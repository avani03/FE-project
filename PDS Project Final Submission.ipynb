{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING THE FUEL ECONOMY AND EMISSIONS OF VEHICLES IN U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INTRODUCTION\n",
    "\n",
    "To make informed decisions about travel and vehicle purchase, consumers need unbiased and accurate information of the fuel economy. Fuel economy estimates by Environmental Protection Agency (EPA) provides a standard for users to make more informed decision while purchasing a new vehicle. It provides a reliable basis for comparing different vehicles in terms of their fuel economy estimates and select the most fuel-efficient option. The estimates are based on laboratory testing under standardized conditions to enable levelized comparison. The subsequent estimates of fuel cost are evaluated using the assumptions on the amount of miles travelled and its split into highway and city travel. In addition to the fuel economy, the tests also suggests the corresponding tailpipe CO2 emissions. EPA has also defined a scale from 1 to 10 for emissions rating level called as EPA Smog Rating  (https://www.epa.gov/greenvehicles/smog-rating) with 10 being the cleanest vehicle. Hence, this database generated as a result of fuel economy testing by EPA provides a good guideline for mapping the vehicle to its fuel consumption and emissions related information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MOTIVATION\n",
    "\n",
    "Transportation sector in US accounts for 28% of the total energy consumption (Source: EIA).  Petroleum based products form the major component of fuel sources used in transportation as compared to natural gas and electricity. Although, hybrid vehicles is attaining popularity contributing to the savings in overall fuel consumption. This is further improved by the growing use of electric cars in the country. Despite the advances, there is still large scope of improving the efficiency in fuel economy. The key step would be to make an informed choice while selecting the vehicle in terms of its fuel economy. \n",
    "Different vehicle parameters impact the fuel economy and hence a suitable guideline is required to understand how to relate them to fuel economy. \n",
    "\n",
    "The standard mpg value of the car is reported by the manufacturer based on standard testing procedures. However, the actual fuel economy obtained by the user can be different from the standard value depending on the conditions in which they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROJECT OBJECTIVE\n",
    "The database prepared by EPA on mapping the different types of vehicles with their fuel economy and emissions information is used as the database to build a supervised learning model. The aim of the model is:\n",
    "- How different type of input features (microscopic features and aesthetic features) can be used to predict the actual fuel economy of the vehicle?\n",
    "- How to provide a guideline for user to select the most fuel-efficient vehicle based on his requirements on basic parameters like drive axle, vehicle class and fuel type?\n",
    "\n",
    "Different datasets such as vehicles information, emissions data and user’s MPG value records are merged to train the model and achieve the project objective. \n",
    "Hence, this project aims at providing this guideline to map the different vehicles to its corresponding fuel economy that is actually obtained by user.\n",
    "\n",
    "\n",
    "#### OVERVIEW OF DATA SOURCES\n",
    "\n",
    "The data for the project is sourced from EPA’s fuel economy website. Three different datasets are merged to obtain the required dataset for the modeling purposes. \n",
    "The first dataset is ‘vehicles.csv’ that contains a database of more than 30,000 vehicle types. It contains information related to the specifications of the car, engine type, fuel economy, annual fuel consumption among others indexed by a ‘vehicle id’. The car specifications include the make, model and class while the engine specifications includes cylinders, piston displacement, transmission and engine description. In addition to the variables defining the car/engine specifications, fuel related information is provided such as fuel type, fuel consumption and its fuel economy. The fuel related information is further divided into fueltype1 and fueltype2 for hybrid engines. Hence, the associated columns such as ‘barrels08’ or ‘CO2 tailpipe emissions’ are separated into two different columns for each fuel type. \n",
    "\n",
    "\n",
    "The second dataset  is ‘emissions.csv’ that contains the emissions information for the vehicles types listed in the vehicles dataset above. The emissions are reported in terms of EPA Score Rating values. This score reflects the CO2 tailpipe emissions as suggested by the tests conducted in standard conditions. The scale goes from 1-10 with 10 being the cleanest. \n",
    "\n",
    "\n",
    "The third dataset is ‘yourMpgVehicle.csv’ that contains the actual fuel economy information reported by the users for the corresponding vehicle type. This provides multiple mpg values reported by user for a particular vehicle id.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA PRE-PROCESSING\n",
    "\n",
    "The first two datasets are downloaded directly from the website (http://www.fueleconomy.gov/feg/ws/index.shtml) while the third dataset on ‘yourMpg’ values are extracted through an API. The XML objects corresponding to each vehicle id were parsed to fetch the corresponding information related to user’s MPG value. All three datasets are merged as inner-join such that the vehicles with missing information are excluded from the dataset.\n",
    "Each of the variables are analyzed to limit the number of variables and curate for modeling purposes. The fuel related columns are combined into one column by combining the information for both the fuel types in the case of secondary fuel in car. The rows with crucial missing information are removed from the database. Since, we have a supervised dataset, this contains both input and output variables of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_INPUT VARIABLES_:\n",
    "\n",
    "- Make\n",
    "- Model\n",
    "- Year\n",
    "- Engine Cylinders\n",
    "- Piston displacement\n",
    "- Drive Axle Type\n",
    "- Fuel Type\n",
    "- Transmission (Manual/Automatic)\n",
    "- Vehicle Class\n",
    "\n",
    "\n",
    "_OUTPUT VARIABLES_:\n",
    "\n",
    "- Fuel Cost\n",
    "- Barrels (petroleum) consumed\n",
    "- User’s fuel economy value\n",
    "- EPA smog rating score\n",
    "\n",
    "Among the different output variables, fuel cost, barrels consumed and EPA smog rating score are standard reported averages for the combination of variables, the model is built to predict the actual fuel economy value reported by the user. The database extracted from ‘yourMpg’ API contains multiple records for the same vehicle type and hence provides a range of mpg values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE SELECTION\n",
    "The csv database for ‘vehicles’ and ‘emissions’ have 83 and 8 columns respectively. The next task we undertook is to select the relevant set of the features. The first method of selection was a simple brainstorming over the relevance of features w.r.t. to the problem we are trying to solve :\n",
    "- In first iteration we eliminated all the columns that were not contributing to any useful information. We validated this step by referring to the documentation available on fueleconomy.gov. <https://www.fueleconomy.gov/feg/pdfs/guides/FEG2017.pdf>\n",
    "- Step (1) helped us reduce the number of attributes to be analyzed further to 45. Next, we removed all those attributes that had more than 50% of the data missing (Nan Tolerance Threshold).\n",
    "- Finally, we were left with 17 attributes that deemed most relevant to the project scope.\n",
    "\n",
    "We plotted the distribution, correlation and spread plots in a matrix for subsets of the features to understand how they relate with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s14.postimg.org/41q00wwsh/Matrix.png\",width=600,height=600>](https://s14.postimg.org/41q00wwsh/Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEATURE EXTRACTION:\n",
    "The key step for data modeling is to select the right features that are uncorrelated to each other and correlates strongly to the output variables being considered. For this purpose, we are exploring the possibility of reducing the dimensionality of the features.\n",
    "\n",
    "Preparing the data for dimensionality reduction:\n",
    "The major hurdle in data preprocessing was handling the categorical variables. All the variables selected are categorical and hence needs to be pre-processed before passing to feature extraction/ regression models. There are two approaches that can be used for this purpose as described below,\n",
    "\n",
    "- Perform one-hot encoding of all the categorical variables forming columns for every category and assigning binary values to it.\n",
    "- Convert categorical variable to numerical codes and then mapped to output variable.\n",
    "\n",
    "In this project, one hot encoding approach is used to transform the input variables. Feature extraction is then carried out by Lasso and ExtraTreesClassifier models to select the relevant features for modeling purpose. The dataset with fewer features that are selected as a result of these models was passed to the Support Vector Regression model to predict the actual fuel ecomony value. However, these attempts were rendered unsuccessful as the data was too noisy and hence the regression prediction was not very accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pred1 = Lasso(normalize=True)\n",
    "pred1 = SelectFromModel(pred1)\n",
    "pred1 = pred1.fit(X1,y1)\n",
    "X_new1 = pred1.transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf  = ExtraTreesClassifier()\n",
    "clf = clf.fit(X1,y1)\n",
    "m = SelectFromModel(clf, prefit=True)\n",
    "X_new2 = m.transform(X1)\n",
    "print X_new2.shape\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting a regression problem to classification:\n",
    "\n",
    "Due to the non-suitable results, the project objective is modified to perform classification of vehicle types into different range of MPG values. Hence, a histogram is first plotted to observe the distribution of the mpg values reported by user along with the reported standard values. Some erratically high user reported mpg records are removed from the dataframe before performing the feature extraction and final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(df.comb08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelnew = model[model.mpg < 70]\n",
    "plt.hist(modelnew.mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the frequency distribution, the user-reported MPG values are divided into 5 different range categories:\n",
    "(0-10, 10-15, 15-25, 25-40, 40-70)\n",
    "These are further encoded to a numeric value while training the model or running models on dataframe for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = [0, 10, 15, 25, 40, 70]\n",
    "group_names = [0,1,2,3,4]\n",
    "modelnew['mpgcat'] = pd.cut(modelnew['mpg'], bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL CLASSIFICATION MODEL\n",
    "\n",
    "#### Feature Extraction\n",
    "\n",
    "The variables selected from manual filtering and pre-processing are converted into one hot encoding and further truncated using the feature extraction model named 'ExtraTreesClassifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf  = ExtraTreesClassifier()\n",
    "clf = clf.fit(X_final,y_final)\n",
    "m = SelectFromModel(clf, prefit=True)\n",
    "X_final_feature_selected = m.transform(X_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "Now, using the selected features, different classification models are evaluated to chose the most fitting one. The 'ModelSelector' function performs k-fold cross validation for the model to evaluate the average accuracy of each model type. We have used KFold function from scikit-learn to perform k-fold cross-validation for evaluation of models and used accuracy as the metric to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ModelSelector(model_clf, df):\n",
    "    X_final = np.array(df)\n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    accuracy_list = []\n",
    "    count = 0\n",
    "    for train, test in kf.split(X_final):\n",
    "        print count\n",
    "        X_train, X_test, y_train, y_test = X_final[train], X_final[test], y_final[train],y_final[test]\n",
    "        \n",
    "        clf = model_clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        accuracy = float(sum(np.array(y_test)==y_pred))/float(len(y_test))\n",
    "        count += 1\n",
    "        print accuracy\n",
    "        accuracy_list.append(accuracy)\n",
    "    \n",
    "    return np.average(accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluated 4 models for the classification task. We did not perform GridSearch for the evaluation of hyperparameters, instead used the documentation provided by scikit-learn to choose use the default hyperparameters.\n",
    "\n",
    "We choose our first model to evaluate as Adaboost Classifier. Since the Adaboost begins by fixing a classifier on the original dataset provided and then keeps fitting the additional copies of classfier on the same dataset by adjusting the weights of the incorrectly classified instances, hence in every iteration focusing on the difficult examples to classify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the Adaboost CLassifier\n",
    "model_select = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=600,\n",
    "    learning_rate=1)\n",
    "adaboost_accuracy = ModelSelector(model_select, X_final_feature_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, after evaluating the model, we got an average accuracy of 60.1%. This was lesser than the average accuracy we had expected. After reading through the literature available for Adaboost, we found that this classifer is sensitive to noisy data and is computationally expensive for higher dimensions datasets. Since our data set has mostly categorical datasets, we had to conert them to dummy variables. Because of this, we introduced a lot of sparseness in the data set and potentially some noise, which could be the pivotal reason for poor performance of this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the SGDClassifier CLassifier\n",
    "model_select = linear_model.SGDClassifier(alpha=0.001, n_iter=100,loss='modified_huber')\n",
    "SGD_accuracy = ModelSelector(model_select, X_final_feature_selected)\n",
    "np.average(SGD_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Accuracy for SGD classifier: 43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneVsRest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the OneVSRest CLassifier\n",
    "model_select = OneVsRestClassifier(LinearSVC(random_state=0))\n",
    "onevsrest_accuracy = ModelSelector(model_select, X_final_feature_selected)\n",
    "np.average(onevsrest_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Accuracy for OnevsRest Classifier: 53%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forests are built by boosting and bagging methods and performs well on highly dimensional and categorical data, we expected that it would perform nicely for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the RandomForest CLassifier\n",
    "model_select = RandomForestClassifier(n_estimators=50)\n",
    "onevsrest_accuracy = ModelSelector(model_select, X_final_feature_selected)\n",
    "print np.average(onevsrest_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We were able to achieve average accuracy of 75% on Random Forest Classifier, making it our final model for training the complete sample. This was the maximum accuracy achieved for classification and is hence used for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY FRAMEWORK FOR USER GUIDELINE\n",
    "\n",
    "This model is used in the framework for providing suitable guidelines to the user for selection of fuel efficient vehicle. The user can make a selection of attributes from drive axle type, fuel type and vehicle class. Based on the requirements selected by user, the function gives fuel economy range estimation for all possible options and displays parametric graphs that would guide the user to make the best selection.\n",
    "\n",
    "INPUT BY USER:\n",
    "- Drive Axle Type\n",
    "- Fuel Type\n",
    "- Vehicle Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Query to get the prediction for \n",
    "model_test = modelnew[(modelnew[\"drive\"]==\"Rear-Wheel Drive\") & (modelnew[\"fuelType\"]==\"Regular\") & (modelnew[\"VClass\"]==\"Vans\")].index\n",
    "y_pred = clf.predict(X_final_feature_selected[model_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s23.postimg.org/afaj1fqaj/Whats_App_Image_2016_12_11_at_11_26_08_PM.jpg\">](https://s23.postimg.org/afaj1fqaj/Whats_App_Image_2016_12_11_at_11_26_08_PM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataframe filtered on the user requirements along with their predicted usermpg range. The predicted MPG range can be decoded as given below:\n",
    "\n",
    "- 0 - MPG 0-10\n",
    "- 1 - MPG 10-15\n",
    "- 2 - MPG 15-25\n",
    "- 3 - MPG 25-40\n",
    "- 4 - MPG Above 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s18.postimg.org/9f1jegvih/image.jpg\">](https://s18.postimg.org/9f1jegvih/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the user can select a desired MPG value for the car they would like to buy. For example: desired MPG = 2 (15-25)\n",
    "\n",
    "This will provide guidelines on what make, state, transmission or fueltype are more suitable for the desired characteristics and MPG value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s24.postimg.org/75salg1it/image.jpg\">](https://s24.postimg.org/75salg1it/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s17.postimg.org/g50niq9f3/image.jpg\">](https://s17.postimg.org/g50niq9f3/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s24.postimg.org/vrerjr54l/image.jpg\">](https://s24.postimg.org/vrerjr54l/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://s24.postimg.org/61bik0wl1/image.jpg\">](https://s24.postimg.org/61bik0wl1/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations and Future Scope\n",
    "\n",
    "- The data might not be exhaustive for all the set of possible combinations of variabless that the user can select. So, it might return a null value for that combination set. \n",
    "- Some aesthetic features were missing from this data that can be collected from Edmunds API to get more variables to model from.\n",
    "- Classifiers are trained on default parameters and better hyperparameters can be set in future work that can yield higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
